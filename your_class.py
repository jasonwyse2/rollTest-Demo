#coding:utf-8
import base_class as base
import numpy as np
import pandas as pd
import os
from data.datalib.slide_Htargets_datasets import get_datasets_2
from data.datalib.slide_Htargets_datasets import get_balanced_shuffled_datasets
from keras.utils.np_utils import to_categorical
from keras.layers import Input, BatchNormalization,Dropout,Convolution2D, Dense,Flatten,merge
from keras.regularizers import l2
from keras.optimizers import Adadelta,Adam,RMSprop
from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard
from keras.models import Sequential,load_model,Model
import tool as tool

import keras

class Your_Data(base.Data):
    '''
        write your own data process
    '''
    def __init__(self, data_parameters_obj):
        self._data_parameters_obj = data_parameters_obj

    def __get_dataFileName(self):
        parameter_dict = self._data_parameters_obj._args
        dataType = parameter_dict['dataType']
        startTime, endTime, lookBack_from_endTime, valid_lookBack = tool.get_start_end_lookback(parameter_dict)
        start_str = startTime if not startTime == '' else 'NoStartTime'
        lookBack_from_endTime_str = str(lookBack_from_endTime) if not str(lookBack_from_endTime) == '' else 'NoLookBack'
        valid_lookBack_str = str(valid_lookBack)
        code_wind = parameter_dict['code_wind']
        simulativeCloseSeries_directory = parameter_dict['simulativeCloseSeries_directory']
        postfix = parameter_dict['dataFile_postfix']
        tool.make_directory(simulativeCloseSeries_directory)
        name = simulativeCloseSeries_directory + code_wind + '-' + start_str + '-' + endTime + '-' \
               + lookBack_from_endTime_str + '-' + valid_lookBack_str + '-' + dataType
        fileName = name + postfix
        return fileName
    def _run(self):
        #self._data_dict = self._prepare_data()
        self._prepare_trainValid_data()

    def __read_mat_from_csv(self,csv_path):
        train_start_time = self._data_parameters_obj._args['train_start_time']
        train_end_time = self._data_parameters_obj._args['train_end_time']
        raw_df = pd.read_csv(csv_path)
        date_list = np.array(raw_df.date.tolist()).astype(np.str)
        start_idx = np.where(date_list >= train_start_time)[0][0]
        end_idx = np.where(date_list <= train_end_time)[0][-1]
        raw_sample = raw_df.iloc[:, 1:].as_matrix()
        raw_sample_mat = raw_sample[start_idx:end_idx + 1, :]
        return raw_sample_mat

    def __split_train_valid(self,raw_sample_mat,valid_lookBack_from_endTime=0):

        train_x_list, train_y_list = [], []
        valid_x_list, valid_y_list = [], []
        valid_lookBack_from_endTime = self._data_parameters_obj._args['valid_lookBack_from_endTime']
        closeSeries_num = raw_sample_mat.shape[1]
        for i in range(closeSeries_num):
            close_array = raw_sample_mat[:, i]
            pct = np.diff(close_array) / close_array[:-1]
            close_array = close_array[1:]
            x, y, filtered_close_for_use, close_for_use = get_datasets_2(close_array, pct, self._data_parameters_obj._args)
            train_x0, train_y0 = x[:-valid_lookBack_from_endTime], y[:-valid_lookBack_from_endTime]
            valid_x0, valid_y0 = x[-valid_lookBack_from_endTime:], y[-valid_lookBack_from_endTime:]
            train_x_list.append(train_x0)
            train_y_list.append(train_y0)
            valid_x_list.append(valid_x0)
            valid_y_list.append(valid_y0)
        return train_x_list, train_y_list, valid_x_list, valid_y_list

    def _prepare_trainValid_data(self):
        self._data_parameters_obj._args['dataType'] = 'train_valid'
        df = tool.get_daily_data(self._data_parameters_obj._args)
        simulativeCloseSeries_num = self._data_parameters_obj._args['simulativeCloseSeries_num']
        # each column in 'simulativeCloseSeries_df' is a simulative close series, which can be generated by different generator
        simulativeCloseSeries_df = tool.simulative_close_generator(df.index, df.close, simulativeCloseSeries_num)
        simulativeCloseSeries_fileName = tool.get_fileName_by_startEndLookback(self._data_parameters_obj._args)
        if not os.path.exists(simulativeCloseSeries_fileName):
            simulativeCloseSeries_df.to_csv(simulativeCloseSeries_fileName, encoding='utf-8')
        raw_sample_mat = simulativeCloseSeries_df.as_matrix()
        train_x_list, train_y_list, valid_x_list, valid_y_list = self.__split_train_valid(raw_sample_mat)
        train_x_ndarray, train_x_label_ndarray = np.row_stack((train_x_list)), np.hstack((train_y_list))
        valid_x_ndarray, valid_x_label_ndarray = np.row_stack((valid_x_list)), np.hstack((valid_y_list))

        print 'train_x_ndarray', train_x_ndarray.shape
        balancedShuffled_train_x, balancedShuffled_train_y = get_balanced_shuffled_datasets(train_x_ndarray,
                                                                                            train_x_label_ndarray)
        print 'balancedShuffled_train_x.shape', balancedShuffled_train_x.shape
        balancedShuffled_valid_x, balancedShuffled_valid_y = get_balanced_shuffled_datasets(valid_x_ndarray,
                                                                                            valid_x_label_ndarray)
        print 'balancedShuffled_valid_x.shape,', balancedShuffled_valid_x.shape
        self._train_x, self._train_y = balancedShuffled_train_x, balancedShuffled_train_y
        self._valid_x, self._valid_y = balancedShuffled_valid_x, balancedShuffled_valid_y

    def _prepare_test_data(self):
        self._data_parameters_obj._args['dataType'] = 'test'
        df = tool.get_daily_data(self._data_parameters_obj._args)
        close_array = np.array(df['close'].tolist())
        pct = np.diff(close_array) / close_array[:-1]
        self._test_x, self._test_y, self._test_filtered_close_for_use, self._test_close_for_use = get_datasets_2(close_array, pct,self._data_parameters_obj._args)


def _prepare_data(self):

        df = tool.get_daily_data(self._data_parameters_obj._args)
        dataType = self._data_parameters_obj._args['dataType']

        simulativeCloseSeries_num = self._data_parameters_obj._args['simulativeCloseSeries_num']
        # each column in 'simulativeCloseSeries_df' is a simulative close series, which can be generated by different generator
        simulativeCloseSeries_df = tool.simulative_close_generator(df.index, df.close, simulativeCloseSeries_num)
        simulativeCloseSeries_fileName = tool.get_fileName_by_startEndLookback(self._data_parameters_obj._args)
        if not os.path.exists(simulativeCloseSeries_fileName):
            simulativeCloseSeries_df.to_csv(simulativeCloseSeries_fileName,encoding='utf-8')
        raw_sample_mat = simulativeCloseSeries_df.as_matrix()
        train_x_list, train_y_list, valid_x_list, valid_y_list = self.__split_train_valid(raw_sample_mat)
        train_x_ndarray, train_x_label_ndarray = np.row_stack((train_x_list)), np.hstack((train_y_list))
        valid_x_ndarray, valid_x_label_ndarray = np.row_stack((valid_x_list)), np.hstack((valid_y_list))

        print 'train_x_ndarray', train_x_ndarray.shape
        balancedShuffled_train_x, balancedShuffled_train_y = get_balanced_shuffled_datasets(train_x_ndarray,
                                                                                            train_x_label_ndarray)
        print 'balancedShuffled_train_x.shape', balancedShuffled_train_x.shape
        balancedShuffled_valid_x, balancedShuffled_valid_y = get_balanced_shuffled_datasets(valid_x_ndarray,
                                                                                            valid_x_label_ndarray)
        print 'balancedShuffled_valid_x.shape,', balancedShuffled_valid_x.shape
        self._train_x, self._train_y = balancedShuffled_train_x, balancedShuffled_train_y
        self._valid_x, self._valid_y = balancedShuffled_valid_x, balancedShuffled_valid_y


class Your_Model(base.Model):
    '''
        write your own model
    '''
    def __init__(self, model_parameters_obj):
        self._model_parameters_obj = model_parameters_obj
    def _run(self):
        print('###################### model is running ######################')
        self._train()
        self._evaluate()
        self._test()
    def __CNN_block(self, out, filter_raw, filter_col, nb_filter):
        out_signal_layer = Convolution2D(nb_filter, filter_raw, filter_col, init='normal', activation='relu',
                                         border_mode='valid', W_regularizer=l2(0.001), subsample=(1, 1),
                                         dim_ordering='tf', bias=True)  # ,W_constraint=maxnorm(m=1.2),
        out_signal = out_signal_layer(out)
        out_signal = BatchNormalization(axis=3)(out_signal)
        # out_signal = Activation('relu')(out_signal)
        out_signal = Dropout(0.1)(out_signal)

        out_put_shape = out_signal_layer.output_shape
        return out_signal

    def __get_tenserflow_inputOutput(self):
        train_num = self._model_parameters_obj._args['train_num']
        batch_size = self._model_parameters_obj._args['batch_size']
        nb_classes = self._model_parameters_obj._args['nb_classes']
        save_name = 'a.h5'
        parameter_dict = self._model_parameters_obj._args['data_obj']._data_parameters_obj._args
        data_obj = self._model_parameters_obj._args['data_obj']
        train_x, train_y = data_obj._train_x, data_obj._train_y
        valid_x, valid_y = data_obj._valid_x, data_obj._valid_y

        data_fit_example_shape = train_x.shape
        train_labels = to_categorical(train_y, num_classes=nb_classes)  # ***
        valid_labels = to_categorical(valid_y, num_classes=nb_classes)  # ***
        Input_shape = (1, data_fit_example_shape[2], data_fit_example_shape[3])
        nb_input = data_fit_example_shape[1]
        nb_layer = nb_input - 1

        cov_list, input_list, output_list = [], [], []
        for i in range(nb_input):
            input = Input(shape=Input_shape)
            input_list.append(input)
            noise = BatchNormalization(axis=1)(input)
            cov = self.__CNN_block(noise, 1, 1, 16)
            cov_1 = self.__CNN_block(cov, 1, 1, 16)
            x_out = Flatten()(cov)
            cov_list.append(cov_1)
            out = Dense(nb_classes, activation='softmax')(x_out)
            output_list.append(out)
        out = merge(cov_list, mode='concat', concat_axis=1)
        for j in range(nb_layer):
            out = self.__CNN_block(out, 2, 1, 16)
        out_flat = Flatten()(out)
        out = Dense(nb_classes, activation='softmax')(out_flat)
        output_list.append(out)

        return input_list, output_list

    def __from_x_to_xList(self,x,x_lables):
        x_list = []
        labels_list = [x_lables]
        nb_input = x.shape[1]
        for i in range(nb_input):
            x_list.append(x[:, [i], :, :])
            labels_list.append(x_lables)
        return x_list, labels_list
    def __get_modelFileName(self):

        data_obj = self._model_parameters_obj._args['data_obj']
        parameter_dict = data_obj._data_parameters_obj._args
        startTime, endTime, lookBack_from_endTime, valid_lookBack = tool.get_start_end_lookback(parameter_dict)
        start_str = startTime if not startTime == '' else 'StartNotGiven'
        lookBack_from_endTime_str = str(lookBack_from_endTime) if not '' else 'LookBackNotGiven'
        valid_lookBack_str = str(valid_lookBack) if not str(valid_lookBack)=='' else '-'
        code_wind = parameter_dict['code_wind']
        modelFile_saveDirectory = self._model_parameters_obj._args['modelFile_saveDirectory']
        tool.make_directory(modelFile_saveDirectory)
        modelFile_postfix = self._model_parameters_obj._args['modelFile_postfix']
        modelName = modelFile_saveDirectory+code_wind+'-'+start_str+'-'+endTime+'-'\
                    +lookBack_from_endTime_str + '-' + valid_lookBack_str + modelFile_postfix

        return modelName

    def _get_modelInput_from_generalInput(self,x,y):

        nb_classes = self._model_parameters_obj._args['nb_classes']
        y_categorical = to_categorical(y, num_classes=nb_classes)
        x_list, y_categorical_list = self.__from_x_to_xList(x, y_categorical)
        return x_list, y_categorical_list

    def _train(self):
        data_obj = self._model_parameters_obj._args['data_obj']
        self._train_x_list, self._train_y_categorical_list = self._get_modelInput_from_generalInput(data_obj._train_x, data_obj._train_y)
        self._valid_x_list, self._valid_y_categorical_list = self._get_modelInput_from_generalInput(data_obj._valid_x, data_obj._valid_y)
        tenserflowInput_list, tenserflowOutput_list= self.__get_tenserflow_inputOutput()
        model = Model(input=tenserflowInput_list, output=tenserflowOutput_list)
        # adam = Adam(lr=0.000001)
        nb_input = data_obj._train_x.shape[1]

        rmsprop = RMSprop(lr=0.001, rho=0.9, decay=0.99)
        loss_weights = [0.33]* nb_input
        loss_weights.append(1)
        model.compile(loss='categorical_crossentropy', optimizer=rmsprop,
                      loss_weights=loss_weights,
                      metrics=['accuracy'])  # set loss function and optimizer

        modelFile_fullName = self.__get_modelFileName()
        modelcheck = ModelCheckpoint(modelFile_fullName, monitor='val_loss', save_best_only=True)
        tensorboard = TensorBoard(log_dir='tensorboard', histogram_freq=0, write_graph=True)
        train_num = self._model_parameters_obj._args['train_num']
        batch_size = self._model_parameters_obj._args['batch_size']
        #verbose=1: dynamicly show the progress, verbose=0: show nothing
        model.fit(self._train_x_list, self._train_y_categorical_list, batch_size=batch_size,nb_epoch=train_num,verbose=1,
                  validation_data=(self._valid_x_list, self._valid_y_categorical_list),callbacks=[modelcheck, tensorboard])
        self._model = model

    def _load_model_from_file(self):
        modelFile_fullName = self.__get_modelFileName()
        self._model = keras.models.load_model(modelFile_fullName)

    def _loadModel_evaluate(self):
        modelFileName = self.__get_modelFileName()
        if os.path.exists(modelFileName):
            self._model = keras.models.load_model(modelFileName)
        self._evaluate()

    def _evaluate(self):
        evaluate_batch_size = self._model_parameters_obj._args['evaluate_batch_size']
        evaluate_verbose = self._model_parameters_obj._args['evaluate_verbose']
        model = self._model
        data_obj = self._model_parameters_obj._args['data_obj']
        idx = data_obj._train_x.shape[1]
        train_result = model.evaluate(self._train_x_list, self._train_y_categorical_list, batch_size=evaluate_batch_size, verbose=evaluate_verbose)
        evaluate_result = model.evaluate(self._valid_x_list, self._valid_y_categorical_list, batch_size=evaluate_batch_size, verbose=evaluate_verbose)

        print('Train loss: %f, Train acc: %f, Valid loss: %f, Valid acc: %f') % (
            train_result[idx-1], train_result[2*idx-1], evaluate_result[idx-1], evaluate_result[2*idx-1])

        predict_valid_y_categorical = model.predict(self._valid_x_list)
        predict_valid_y_toInt = np.argmax(predict_valid_y_categorical[-1], axis=1)
        y_true =np.array(data_obj._valid_y)
        y_predict = np.array(predict_valid_y_toInt)
        print('valid confusion matrix')
        confuse_df = tool.confuse_matrix(y_true, y_predict)
        confu = confuse_df.as_matrix()
        pred_acc_per_class = np.diag(confu).astype(np.float) / np.sum(confu, axis=0)
        print 'predict_accuracy_per_class:'
        print pred_acc_per_class
        print 'Average accuracy:', np.mean(pred_acc_per_class)

    def _test(self):
        data_obj = self._model_parameters_obj._args['data_obj']
        data_obj._prepare_test_data()
        test_x_list, test_y_categorical_list = self._get_modelInput_from_generalInput( data_obj._test_x, data_obj._test_y)

        predict_test_y_categorical = self._model.predict(test_x_list)
        predict_test_y_toInt = np.argmax(predict_test_y_categorical[-1], axis=1)
        y_true = np.array(data_obj._test_y)
        y_predict = np.array(predict_test_y_toInt)
        print('test confusion matrix')
        confuse_df = tool.confuse_matrix(y_true, y_predict)
        confu = confuse_df.as_matrix()
        pred_acc_per_class = np.diag(confu).astype(np.float) / np.sum(confu, axis=0)
        print 'predict_accuracy_per_class:'
        print pred_acc_per_class
        print 'Average accuracy:', np.mean(pred_acc_per_class)
        self._model_parameters_obj._args['modelFile_postfix'] = '.csv'
        testResultFileName = self.__get_modelFileName()
        confuse_df.to_csv(testResultFileName)

